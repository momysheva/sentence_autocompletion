{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85909301",
   "metadata": {},
   "source": [
    "The whole code is original, the only thing we were referring to is Perplexity calculation from HuggingFace link below: https://huggingface.co/docs/transformers/en/perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37978fbe",
   "metadata": {},
   "source": [
    "# 1. Dataset creation and preprocessing\n",
    "First of all, we needed to compose a dataset of size suitable for our task and our computational resources availability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877d273a",
   "metadata": {},
   "source": [
    "Here we download the dataset of 70k books from HuggingFace repository of \"manu/project_gutenberg\" and choose only English books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9361fba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the English utenberg dataset with streaming\n",
    "ds = load_dataset(\"manu/project_gutenberg\", split=\"en\", streaming=True)\n",
    "\n",
    "# Print the first sample to see the structure\n",
    "print(next(iter(ds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce46cc8d",
   "metadata": {},
   "source": [
    "### Create Raw Dataset\n",
    "Create a new dataset of 200 books by saving them in chunks to avoid memeory overflow. Then saved the raw dataset in txt file called \"combined_gutenberg_dataset_small.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516e3bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "# Specify the path to save the combined text file\n",
    "output_file = \"combined_gutenberg_dataset_small.txt\"\n",
    "\n",
    "# Initialize an empty string to store the entire text\n",
    "combined_text = \"\"\n",
    "\n",
    "# Use tqdm to display progress\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for i, sample in enumerate(tqdm(ds, desc=\"Downloading & Processing Books\", unit=\"books\")):\n",
    "        text = sample['text']\n",
    "        combined_text += text + \"\\n\\n\"  # Adding newline for separation between books\n",
    "        \n",
    "        # Ыave in chunks to avoid memory issues\n",
    "        if (i + 1) % 500 == 0:\n",
    "            f.write(combined_text)\n",
    "            combined_text = \"\"  # Clear the buffer to avoid memory overflow\n",
    "\n",
    "        if (i + 1) == 200:\n",
    "            break\n",
    "\n",
    "    # Write remaining text to file\n",
    "    f.write(combined_text)\n",
    "\n",
    "print(f\"\\nAll books have been saved successfully to '{output_file}'!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46016f5a",
   "metadata": {},
   "source": [
    "Here we load the raw dataset and then start preprocessing part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70d316d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load text files\n",
    "def load_text(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d386957",
   "metadata": {},
   "outputs": [],
   "source": [
    "gutenberg_text = load_text(\"combined_gutenberg_dataset_small.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1f6d48",
   "metadata": {},
   "source": [
    "Import and download nltk library functions for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e726a599",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "# Set a custom directory for NLTK data\n",
    "nltk_data_path = os.path.abspath(\"./nltk_data\")\n",
    "\n",
    "os.makedirs(nltk_data_path, exist_ok=True)\n",
    "nltk.data.path.append(nltk_data_path)\n",
    "nltk.download('punkt', download_dir=nltk_data_path)\n",
    "\n",
    "# Verify the tokenizer works\n",
    "tokens = nltk.word_tokenize(\"This is a test sentence.\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba3065f",
   "metadata": {},
   "source": [
    "### Preprocess and save the new dataset\n",
    "Create the whole preprocessing function for pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35baef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_text(data, min_length=5, max_length=100):\n",
    "    # Fix incorrect line breaks to merge lines\n",
    "    data = re.sub(r\"\\n([a-z])\", r\" \\1\", data)\n",
    "    \n",
    "    # Merge sentences within quotation marks that are split across multiple lines\n",
    "    data = re.sub(r'(\".*?)(\\n\\s*)(.*?\")', r'\\1 \\3', data, flags=re.DOTALL)\n",
    "    \n",
    "    # Remove Project Gutenberg header and footer\n",
    "    data = re.sub(r\"(START OF THE PROJECT GUTENBERG EBOOK.*?\\n)|(\\*\\*\\*START OF.*?\\*\\*\\*)\", \"\", data, flags=re.IGNORECASE | re.DOTALL)\n",
    "    data = re.sub(r\"(End of the Project Gutenberg EBook.*?\\n)|(\\*\\*\\*END OF.*?\\*\\*\\*)\", \"\", data, flags=re.IGNORECASE | re.DOTALL)\n",
    "    \n",
    "    # Remove Project Gutenberg license info and URLs\n",
    "    data = re.sub(r\"http[s]?://\\S+\", \"\", data)  # Remove URLs\n",
    "    data = re.sub(r\"Project Gutenberg.*?License\", \"\", data, flags=re.IGNORECASE | re.DOTALL)\n",
    "    \n",
    "    # Remove metadata (e.g. Title, Author, Release Date, etc.)\n",
    "    data = re.sub(r\"(Title:.*?(\\n|$))\", \"\", data, flags=re.IGNORECASE)\n",
    "    data = re.sub(r\"(Author:.*?(\\n|$))\", \"\", data, flags=re.IGNORECASE)\n",
    "    data = re.sub(r\"(Release Date:.*?(\\n|$))\", \"\", data, flags=re.IGNORECASE)\n",
    "    data = re.sub(r\"(Language:.*?(\\n|$))\", \"\", data, flags=re.IGNORECASE)\n",
    "    data = re.sub(r\"(Character set encoding:.*?(\\n|$))\", \"\", data, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Remove transcriber's notes and footnotes\n",
    "    data = re.sub(r\"Transcriber's note:.*?(\\n|$)\", \"\", data, flags=re.IGNORECASE)\n",
    "    data = re.sub(r\"\\[.*?\\]\", \"\", data)  # Remove inline footnotes or comments\n",
    "    \n",
    "    # Remove page numbers and extra line breaks\n",
    "    data = re.sub(r\"\\b[0-9]+\\b\", \"\", data)\n",
    "    data = re.sub(r\"\\n\\s*\\n\", \"\\n\", data)\n",
    "    \n",
    "    # Sentence Tokenization\n",
    "    sentences = nltk.sent_tokenize(data)\n",
    "    \n",
    "    cleaned_sentences = []\n",
    "    \n",
    "    # Here we preprocess and filter each sentence to ensure higher quality\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip()\n",
    "        \n",
    "        # Remove lines that are likely headings or chapter titles\n",
    "        if re.match(r\"^(CHAPTER|ADDISON|[A-Z\\s]+)$\", sentence):\n",
    "            continue\n",
    "        \n",
    "        # Remove sentences that are mostly uppercase or contain no alphanumeric characters\n",
    "        if sentence.isupper() or not re.search(r\"[a-zA-Z0-9]\", sentence):\n",
    "            continue\n",
    "        \n",
    "        # Remove extra spaces within sentences (NEW ADDITION)\n",
    "        sentence = re.sub(r\"\\s+\", \" \", sentence)\n",
    "        \n",
    "        # Ensure the sentence starts with a capital letter\n",
    "        if not sentence[0].isupper():\n",
    "            continue\n",
    "        \n",
    "        # Remove sentences that are too short or too long\n",
    "        if len(sentence.split()) < min_length or len(sentence.split()) > max_length:\n",
    "            continue\n",
    "        \n",
    "        # Append the valid sentence to the list\n",
    "        cleaned_sentences.append(sentence)\n",
    "    \n",
    "    return cleaned_sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19840fe",
   "metadata": {},
   "source": [
    "Preprocess and save the preprocessed dataset of 200 Gutenberk books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c07d870",
   "metadata": {},
   "outputs": [],
   "source": [
    "gutenberg_sentences = preprocess_text(gutenberg_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c9ee90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "output_file = \"preprocessed_gutenberg_sentences_small.txt\"\n",
    "gutenberg_combined = \"\"\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    with tqdm(total=len(gutenberg_sentences), desc=\"Saving Sentences\", unit=\"sentence\") as pbar:\n",
    "        for i, sentence in enumerate(gutenberg_sentences):\n",
    "            gutenberg_combined += sentence + \"\\n\"\n",
    "\n",
    "            # Write to file every 10,000 sentences to avoid memory issues\n",
    "            if (i + 1) % 10000 == 0:\n",
    "                f.write(gutenberg_combined)\n",
    "                gutenberg_combined = \"\"  # Clear buffer after writing\n",
    "                pbar.update(10000)  # Update progress bar\n",
    "\n",
    "        if gutenberg_combined:\n",
    "            f.write(gutenberg_combined)\n",
    "            pbar.update(len(gutenberg_sentences) % 10000)\n",
    "\n",
    "print(f\"\\nAll sentences have been successfully saved to {output_file}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2c08f7",
   "metadata": {},
   "source": [
    "### Load the preprocessed dataset and split into train and test\n",
    "Here we load the dataset and then we clean it once again by removing some artifcats like special symbols. Also remove short sentence less than 10 words. Then we split and save it into train and test txt files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cb0fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to ombined text file (e.g., Project Gutenberg books)\n",
    "dataset_path = \"../dataset_prep/data/preprocessed_gutenberg_sentences_small.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45446153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the text file\n",
    "with open(dataset_path, 'r', encoding='utf-8') as file:\n",
    "    data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea30075",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Split the text into sentences (assuming '\\n' as sentence separator)\n",
    "sentences = data.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954ce30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(sentence):\n",
    "    # Replace smart quotes with standard quotes\n",
    "    sentence = sentence.replace(\"“\", \"\\\"\").replace(\"”\", \"\\\"\")\n",
    "    sentence = sentence.replace(\"‘\", \"'\").replace(\"’\", \"'\")\n",
    "    \n",
    "    # Remove unwanted characters (keeping basic punctuation and letters)\n",
    "    cleaned = re.sub(r\"[^a-zA-Z0-9\\s.,!?]\", \"\", sentence)\n",
    "    \n",
    "    # Remove extra spaces\n",
    "    cleaned = re.sub(r'\\s+', ' ', cleaned).strip()\n",
    "    \n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22a8f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean sentences and filter out very short ones\n",
    "cleaned_sentences = [clean_text(sentence) for sentence in sentences if len(sentence.strip()) > 0]\n",
    "\n",
    "# Filter out very short sentences (less than 10 words)\n",
    "filtered_sentences = [sentence for sentence in cleaned_sentences if len(sentence.split()) >= 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea864a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into Train (95%) and Test (5%)\n",
    "train_sentences, test_sentences = train_test_split(filtered_sentences, test_size=0.05, random_state=424)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc83bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the split datasets for easy access later\n",
    "with open(\"train.txt\", \"w\", encoding='utf-8') as file:\n",
    "    file.write('\\n'.join(train_sentences))\n",
    "\n",
    "with open(\"test.txt\", \"w\", encoding='utf-8') as file:\n",
    "    file.write('\\n'.join(test_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748a0c5b",
   "metadata": {},
   "source": [
    "# 2. Training and evaluation of models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65b3aea",
   "metadata": {},
   "source": [
    "### First of all we need to define evaluation function for BLEU, BERT and Perplexity\n",
    "\n",
    "NOTE: BLEU score was not eventaully used in most models because initial evaluations showed that BLEU is completely not suitable for our project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f4f05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all libraries\n",
    "import torch\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from huggingface_hub import notebook_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b293d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(model, tokenizer, sentences, device=\"cpu\"):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    nll_sum = 0.0\n",
    "    n_tokens = 0\n",
    "\n",
    "    for sentence in tqdm(sentences, desc=\"Calculating Perplexity\"):\n",
    "        sentence = sentence.strip()\n",
    "        if len(sentence) == 0:\n",
    "            continue\n",
    "        \n",
    "        encodings = tokenizer(sentence, return_tensors=\"pt\")\n",
    "        input_ids = encodings.input_ids.to(device)\n",
    "\n",
    "        max_length = model.config.max_position_embeddings\n",
    "        stride = 512\n",
    "        seq_len = input_ids.size(1)\n",
    "        prev_end_loc = 0\n",
    "\n",
    "        for begin_loc in range(0, seq_len, stride):\n",
    "            end_loc = min(begin_loc + max_length, seq_len)\n",
    "            trg_len = end_loc - prev_end_loc  # Number of tokens to predict\n",
    "            input_chunk = input_ids[:, begin_loc:end_loc]\n",
    "            \n",
    "            target_chunk = input_chunk.clone()\n",
    "            target_chunk[:, :-trg_len] = -100  # Masking tokens\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_chunk, labels=target_chunk)\n",
    "                neg_log_likelihood = outputs.loss.item() * trg_len  # Total NLL for this chunk\n",
    "\n",
    "            # Count valid tokens\n",
    "            num_valid_tokens = (target_chunk != -100).sum().item()\n",
    "            nll_sum += neg_log_likelihood\n",
    "            n_tokens += num_valid_tokens\n",
    "\n",
    "            prev_end_loc = end_loc\n",
    "            if end_loc == seq_len:\n",
    "                break\n",
    "\n",
    "    # Calculate final perplexity\n",
    "    avg_nll = nll_sum / n_tokens  # Average Negative Log-Likelihood\n",
    "    perplexity = torch.exp(torch.tensor(avg_nll))\n",
    "    return perplexity.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced03caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bleu_score(model, tokenizer, sentences, device=\"cpu\", prompt_length=8, max_length=100):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    total_bleu = 0.0\n",
    "    smoothing = SmoothingFunction().method1\n",
    "    num_samples = len(sentences)\n",
    "    \n",
    "    # Initialize progress bar outside the loop\n",
    "    progress_bar = tqdm(sentences, desc=\"Calculating BLEU Score\", leave=True)\n",
    "\n",
    "    for sentence in progress_bar:\n",
    "        sentence = sentence.strip()\n",
    "        if len(sentence) == 0:\n",
    "            continue\n",
    "\n",
    "        # Extract prompt based on a number of words, not characters\n",
    "        words = sentence.split()\n",
    "        \n",
    "        if len(words) <= prompt_length:\n",
    "            continue  # Skip sentences that are too short to provide a valid prompt\n",
    "\n",
    "        prompt = ' '.join(words[:prompt_length])  # Take the first few words as the prompt\n",
    "        actual_continuation = ' '.join(words[prompt_length:])  # The remaining words are the reference text\n",
    "\n",
    "        # Encode the prompt\n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                max_length=max_length,\n",
    "                temperature=1,\n",
    "                top_p=0.8,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        # Decode generated text and isolate the continuation\n",
    "        generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "        # Remove the prompt part from the generated text\n",
    "        if generated_text.startswith(prompt):\n",
    "            generated_continuation = generated_text[len(prompt):].strip()\n",
    "        else:\n",
    "            generated_continuation = generated_text\n",
    "\n",
    "        # Tokenize both reference and generated text for BLEU calculation\n",
    "        reference_tokens = actual_continuation.lower().split()\n",
    "        generated_tokens = generated_continuation.lower().split()\n",
    "\n",
    "        # Calculate BLEU score for the current sentence\n",
    "        if len(reference_tokens) > 0 and len(generated_tokens) > 0:\n",
    "            bleu = sentence_bleu([reference_tokens], generated_tokens, smoothing_function=smoothing)\n",
    "            total_bleu += bleu\n",
    "\n",
    "    avg_bleu = total_bleu / num_samples\n",
    "    return avg_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36396773",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate \n",
    "\n",
    "def calculate_bertscore(model, tokenizer, sentences, device=\"cpu\", prompt_length=8, max_new_tokens=50):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    references = []\n",
    "    predictions = []\n",
    "\n",
    "    progress_bar = tqdm(sentences, desc=\"Calculating BERTScore\", leave=True)\n",
    "\n",
    "    for sentence in progress_bar:\n",
    "        sentence = sentence.strip()\n",
    "        if len(sentence) == 0:\n",
    "            continue\n",
    "\n",
    "        words = sentence.split()\n",
    "        if len(words) <= prompt_length:\n",
    "            continue  # skip too-short examples\n",
    "\n",
    "        prompt = ' '.join(words[:prompt_length])\n",
    "        actual_continuation = ' '.join(words[prompt_length:])\n",
    "\n",
    "        # Encode and generate\n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=1,\n",
    "                top_p=0.8,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "        generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "        # Remove the prompt part\n",
    "        if generated_text.startswith(prompt):\n",
    "            generated_continuation = generated_text[len(prompt):].strip()\n",
    "        else:\n",
    "            generated_continuation = generated_text.strip()\n",
    "\n",
    "        references.append(actual_continuation)\n",
    "        predictions.append(generated_continuation)\n",
    "\n",
    "    bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "    results = bertscore.compute(\n",
    "        predictions=predictions,\n",
    "        references=references,\n",
    "        lang=\"en\",\n",
    "        batch_size=4,\n",
    "        model_type=\"microsoft/deberta-xlarge-mnli\"\n",
    "    )\n",
    "\n",
    "    avg_p = sum(results[\"precision\"]) / len(results[\"precision\"])\n",
    "    avg_r = sum(results[\"recall\"]) / len(results[\"recall\"])\n",
    "    avg_f1 = sum(results[\"f1\"]) / len(results[\"f1\"])\n",
    "\n",
    "    return avg_p, avg_r, avg_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3d747a",
   "metadata": {},
   "source": [
    "This one function is to generate text for qualitative evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c778f181",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, prompt, max_new_tokens=50, temperature=1, top_p=0.8):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e48b39",
   "metadata": {},
   "source": [
    "## 2.1 Training GPT-2 Full-Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3796b3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# ensure we have a pad token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f53af00",
   "metadata": {},
   "source": [
    "We need to split train data to training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7b063e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"text\", data_files={\"all\": \"train.txt\"})[\"all\"]\n",
    "train_texts, val_texts = train_test_split(dataset[\"text\"],\n",
    "                                          test_size=0.1,\n",
    "                                          random_state=182)\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": Dataset.from_dict({\"text\": train_texts}),\n",
    "    \"validation\": Dataset.from_dict({\"text\": val_texts}),\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd97cd3",
   "metadata": {},
   "source": [
    "Tokenization step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed93d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_fn(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_attention_mask=True\n",
    "    )\n",
    "\n",
    "tokenized = dataset_dict.map(\n",
    "    tokenize_fn,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"],\n",
    ")\n",
    "tokenized.set_format(\n",
    "    type=\"torch\",\n",
    "    columns=[\"input_ids\", \"attention_mask\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f78f18",
   "metadata": {},
   "source": [
    "Define training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84585d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./outputs/finetuned_gpt2\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=16,   \n",
    "    learning_rate=5e-4,              \n",
    "    save_steps=5000,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=200,\n",
    "    eval_strategy=\"steps\",        \n",
    "    eval_steps=5000,\n",
    "    fp16=True,\n",
    "    report_to=\"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52950c8f",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d62e98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    eval_dataset=tokenized[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161c1662",
   "metadata": {},
   "source": [
    "Evaluation of the model with test data: Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb17fdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "checkpoint_dir = \".../outputs/finetuned_gpt2/checkpoint-38465\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_dir)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint_dir)\n",
    "model.eval().cuda()\n",
    "\n",
    "# 2) Tokenize your validation data\n",
    "max_length = 128\n",
    "def tokenize_fn(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "ds = load_dataset(\"text\", data_files={\"validation\": \"test.txt\"})\n",
    "tokenized_val = ds[\"validation\"].map(tokenize_fn, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# 3) Create DataLoader\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "dataloader = DataLoader(tokenized_val, batch_size=16, collate_fn=data_collator)\n",
    "\n",
    "# 4) Manually evaluate and compute perplexity\n",
    "losses = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "        batch = {k: v.cuda() for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        losses.append(outputs.loss.item())\n",
    "\n",
    "# 5) Compute perplexity\n",
    "avg_loss = sum(losses) / len(losses)\n",
    "perplexity = math.exp(avg_loss)\n",
    "print(f\"Manual evaluation perplexity: {perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cb92dc",
   "metadata": {},
   "source": [
    "BERT, BLEU and PPL scores Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674f54fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    test_sentences = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152f49e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_score = calculate_bertscore(model, tokenizer, test_sentences[:350], device=\"cuda\", prompt_length=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1134607",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"BERT: {bert_score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45552135",
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity_score = calculate_perplexity(model, tokenizer, test_sentences, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7fe2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Perplexity: {perplexity_score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2792d05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_score = calculate_bleu_score(model, tokenizer, test_sentences[:350], device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207c8ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"BLEU: {bleu_score :.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e471d21",
   "metadata": {},
   "source": [
    "Sentences generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc07b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"When inquisitors punish heretics it is not with the desire to\"\n",
    "reference = \"When inquisitors punish heretics it is not with the desire to destroy them, but that they shall be converted and live.\"\n",
    "generated_sentence = generate_text(model, tokenizer, prompt)\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Generated Text: {generated_sentence}\")\n",
    "print(f\"Reference Text: {reference}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b488baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"The most personally courageous become bullies and\"\n",
    "reference = \"The most personally courageous become bullies and the terror of the community.\"\n",
    "generated_sentence = generate_text(model, tokenizer, prompt)\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Generated Text: {generated_sentence}\")\n",
    "print(f\"Reference Text: {reference}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d04d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"The whisper of the wind that stirred the willows made soft accompaniment of the splash of\"\n",
    "reference = \"The whisper of the wind that stirred the willows made soft accompaniment of the splash of paddle in the stream the birds sang lustily amid the gentle rustle of the garden trees, and when the thrush retired to roost the nightingale took up the tale.\"\n",
    "\n",
    "generated_sentence = generate_text(model, tokenizer, prompt)\n",
    "\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Generated: {generated_sentence}\")\n",
    "print(f\"Reference: {reference}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0043e661",
   "metadata": {},
   "source": [
    "## 2.2 Training GPT-NEO Full-Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b2ffca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"EleutherAI/gpt-neo-125M\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfb4417",
   "metadata": {},
   "source": [
    "Dataset preeparation and Tokenization parts are similiar with GPT-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b7743c",
   "metadata": {},
   "source": [
    "Defining training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9c1856",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Define training arguments.\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gptneo_finetuned\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=16, \n",
    "    learning_rate=5e-4,                          \n",
    "    save_steps=5000,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=200,\n",
    "    eval_strategy=\"epoch\",\n",
    "    fp16=True,\n",
    "    report_to=\"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdf03b5",
   "metadata": {},
   "source": [
    "## 2.3. Train GPT-2 with LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c6fd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "tokenized_datasets = load_from_disk(\"tokenized_datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52063da8",
   "metadata": {},
   "source": [
    "Define LoRa configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d219b9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"c_attn\", \"c_proj\"],  # Targeting the right layers for GPT-2\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze only the LoRA adapter parameters\n",
    "for name, param in model.named_parameters():\n",
    "    if \"lora\" in name:\n",
    "        param.requires_grad = True  # Keep adapters trainable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892ce623",
   "metadata": {},
   "source": [
    "Tokenization step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c619df25",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c6a0b9",
   "metadata": {},
   "source": [
    "Define Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a00c429",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/kaggle/working/distilgpt2-lora\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=16,\n",
    "    save_steps=5000,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=200,\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=5e-4,\n",
    "    fp16=True,  # Use mixed precision\n",
    "    push_to_hub=False,  # Automatically pushes checkpoints to the Hub\n",
    "    report_to=\"none\"\n",
    ")\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442d32f7",
   "metadata": {},
   "source": [
    "Train GPT-Neo with LoRa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb66f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5088f39c",
   "metadata": {},
   "source": [
    "Evaluation by calculating score for quantitative results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d97a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_score = calculate_bertscore(model, tokenizer, test_sentences[:350], device=\"cuda\", prompt_length=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1903db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"BERT: {bert_score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d29ccab",
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity_score = calculate_perplexity(model, tokenizer, test_sentences, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3253ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Perplexity: {perplexity_score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67da67af",
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_score = calculate_bleu_score(model, tokenizer, test_sentences[:350], device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fccc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"BLEU: {bleu_score :.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00998d0e",
   "metadata": {},
   "source": [
    "Getting genertaed sentences for qualitative results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a330a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"When inquisitors punish heretics it is not with the desire to\"\n",
    "reference = \"When inquisitors punish heretics it is not with the desire to destroy them, but that they shall be converted and live.\"\n",
    "generated_sentence = generate_text(model, tokenizer, prompt)\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Generated Text: {generated_sentence}\")\n",
    "print(f\"Reference Text: {reference}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776fcd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"The most personally courageous become bullies and\"\n",
    "reference = \"The most personally courageous become bullies and the terror of the community.\"\n",
    "generated_sentence = generate_text(model, tokenizer, prompt)\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Generated Text: {generated_sentence}\")\n",
    "print(f\"Reference Text: {reference}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6196926c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"The whisper of the wind that stirred the willows made soft accompaniment of the splash of\"\n",
    "reference = \"The whisper of the wind that stirred the willows made soft accompaniment of the splash of paddle in the stream the birds sang lustily amid the gentle rustle of the garden trees, and when the thrush retired to roost the nightingale took up the tale.\"\n",
    "\n",
    "generated_sentence = generate_text(model, tokenizer, prompt)\n",
    "\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Generated: {generated_sentence}\")\n",
    "print(f\"Reference: {reference}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d105cd5c",
   "metadata": {},
   "source": [
    "## 2.4. Train GPT-Neo with LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4831d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"EleutherAI/gpt-neo-125M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2ec86e",
   "metadata": {},
   "source": [
    "LoRa configuration for GPT-Neo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c82898e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "  r=8,\n",
    "  lora_alpha=32,\n",
    "  target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\"],\n",
    "  lora_dropout=0.1,\n",
    "  bias=\"none\",\n",
    "  task_type=TaskType.CAUSAL_LM,\n",
    ")\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e98047",
   "metadata": {},
   "source": [
    "Tokenization step is similar to GPT2 LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088d90c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gptneo-lora\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=4,\n",
    "    save_steps=5000,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=200,\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=5e-4,\n",
    "    fp16=True,  # Use mixed precision\n",
    "    push_to_hub=False,  # Automatically pushes checkpoints to the Hub\n",
    "    report_to=\"none\"\n",
    ")\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "  tokenizer=tokenizer, mlm=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "  model=model,\n",
    "  args=training_args,\n",
    "  train_dataset=tokenized_datasets[\"train\"],\n",
    "  eval_dataset= tokenized_datasets[\"validation\"],\n",
    "  data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df26a49d",
   "metadata": {},
   "source": [
    "Evaluation part\n",
    "\n",
    "Calculate quantitaive scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c14a2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_score = calculate_bertscore(model, tokenizer, test_sentences[:350], device=\"cuda\", prompt_length=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ff903b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"BERT: {bert_score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73361fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity_score = calculate_perplexity(model, tokenizer, test_sentences, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c5d992",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Perplexity: {perplexity_score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdefac0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_score = calculate_bleu_score(model, tokenizer, test_sentences[:350], device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becf47a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"BLEU: {bleu_score :.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ff3281",
   "metadata": {},
   "source": [
    "Get Qualitative Results by text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c92fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"When inquisitors punish heretics it is not with the desire to\"\n",
    "reference = \"When inquisitors punish heretics it is not with the desire to destroy them, but that they shall be converted and live.\"\n",
    "generated_sentence = generate_text(model, tokenizer, prompt)\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Generated Text: {generated_sentence}\")\n",
    "print(f\"Reference Text: {reference}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d029eca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"The most personally courageous become bullies and\"\n",
    "reference = \"The most personally courageous become bullies and the terror of the community.\"\n",
    "generated_sentence = generate_text(model, tokenizer, prompt)\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Generated Text: {generated_sentence}\")\n",
    "print(f\"Reference Text: {reference}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71e6e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"The whisper of the wind that stirred the willows made soft accompaniment of the splash of\"\n",
    "reference = \"The whisper of the wind that stirred the willows made soft accompaniment of the splash of paddle in the stream the birds sang lustily amid the gentle rustle of the garden trees, and when the thrush retired to roost the nightingale took up the tale.\"\n",
    "\n",
    "generated_sentence = generate_text(model, tokenizer, prompt)\n",
    "\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Generated: {generated_sentence}\")\n",
    "print(f\"Reference: {reference}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d73fae",
   "metadata": {},
   "source": [
    "## 2.5 Train Distilled Deepseek-R1 with LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfdc385",
   "metadata": {},
   "source": [
    "Load the DeepSeek-R1-Distill-Qwen-1.5B model from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324fa5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d798070e",
   "metadata": {},
   "source": [
    "Define bnb config for a bit optimized training because the model is too large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01155839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ac3bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb4a4a8",
   "metadata": {},
   "source": [
    "Example of how pre-trained models were evaluated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae811a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select subset for quick evaluation\n",
    "eval_samples = [x['text'] for x in test_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348a4fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Pre-Training Metrics\")\n",
    "base_perplexity = calculate_perplexity(model, tokenizer, eval_samples, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed081f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Base Model Perplexity: {base_perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1147328",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_bleu = calculate_bleu_score(model, tokenizer, eval_samples[:350], device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633eec39",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Base Model BLEU: {base_bleu:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be3e5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_bert = calculate_bertscore(model, tokenizer, test_sentences[:350], device=\"cuda\", prompt_length=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352e822a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Base Model BERT: {base_bert:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12394f7",
   "metadata": {},
   "source": [
    "Here we printed generation results for quantitaive analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e228f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"When inquisitors punish heretics it is not with the desire to\"\n",
    "reference = \"When inquisitors punish heretics it is not with the desire to destroy them, but that they shall be converted and live.\"\n",
    "generated_sentence = generate_text(model, tokenizer, prompt)\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Generated Text: {generated_sentence}\")\n",
    "print(f\"Reference Text: {reference}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e403f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"The most personally courageous become bullies and\"\n",
    "reference = \"The most personally courageous become bullies and the terror of the community.\"\n",
    "generated_sentence = generate_text(model, tokenizer, prompt)\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Generated Text: {generated_sentence}\")\n",
    "print(f\"Reference Text: {reference}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890b9bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"The whisper of the wind that stirred the willows made soft accompaniment of the splash of\"\n",
    "reference = \"The whisper of the wind that stirred the willows made soft accompaniment of the splash of paddle in the stream the birds sang lustily amid the gentle rustle of the garden trees, and when the thrush retired to roost the nightingale took up the tale.\"\n",
    "\n",
    "generated_sentence = generate_text(model, tokenizer, prompt)\n",
    "\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Generated: {generated_sentence}\")\n",
    "print(f\"Reference: {reference}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501fb50f",
   "metadata": {},
   "source": [
    "Here defined LoRA config as for other models above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e71375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA setup\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a647abf9",
   "metadata": {},
   "source": [
    "Here is example of freezing layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16cf410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually enable LoRA params if needed\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad is False and \"lora\" in name:\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b84d042",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    tokenized = tokenizer(\n",
    "        examples['text'],\n",
    "    )\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bee723",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train = train_dataset.map(tokenize_function, batched=True, remove_columns=['text'])\n",
    "tokenized_test = test_dataset.map(tokenize_function, batched=True, remove_columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7bcdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a59f880",
   "metadata": {},
   "source": [
    "Training part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faaccb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/kaggle/working/distil-deepseek-lora\",\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=5,\n",
    "    eval_strategy=\"epoch\",\n",
    "    logging_steps=200,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    save_steps=1000,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,\n",
    "    logging_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1064558b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872cd7e4",
   "metadata": {},
   "source": [
    "Saving model in Hugging Face for safety, explained lower in the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5020fed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "trainer.save_model(\"/kaggle/working/final_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755d9edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to Hugging Face\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c48a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push model\n",
    "model.push_to_hub(\"dudessa/deepseek-1.5b-gutenberg-lora\")\n",
    "tokenizer.push_to_hub(\"dudessa/deepseek-1.5b-gutenberg-lora\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac1078f",
   "metadata": {},
   "source": [
    "This part of the code was used to continue model training after saving it in hugging face. This step was essential to not lose training results after very long training sessions, as online notebooks tend to crash after several hours of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaafa102",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "# Then load your LoRA adapter into it\n",
    "model = PeftModel.from_pretrained(base_model, \"dudessa/deepseek-1.5b-gutenberg-lora\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dudessa/deepseek-1.5b-gutenberg-lora\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b29ba1a",
   "metadata": {},
   "source": [
    "Evaluation after fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964efcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select subset for quick evaluation\n",
    "eval_samples = [x['text'] for x in test_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a900574b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Post-Training Metrics\")\n",
    "post_perplexity = calculate_perplexity(model, tokenizer, eval_samples, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdecbe06",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Post Model Perplexity: {post_perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888c2f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_bleu = calculate_bleu_score(model, tokenizer, eval_samples[:350], device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc72f61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Post Model BLEU: {post_bleu:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069a1de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_bert = calculate_bertscore(model, tokenizer, test_sentences[:350], device=\"cuda\", prompt_length=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d11728",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Post Model BERT: {post_bert:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ada1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"When inquisitors punish heretics it is not with the desire to\"\n",
    "reference = \"When inquisitors punish heretics it is not with the desire to destroy them, but that they shall be converted and live.\"\n",
    "generated_sentence = generate_text(model, tokenizer, prompt)\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Generated Text: {generated_sentence}\")\n",
    "print(f\"Reference Text: {reference}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e03c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"The most personally courageous become bullies and\"\n",
    "reference = \"The most personally courageous become bullies and the terror of the community.\"\n",
    "generated_sentence = generate_text(model, tokenizer, prompt)\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Generated Text: {generated_sentence}\")\n",
    "print(f\"Reference Text: {reference}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0562ffbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"The whisper of the wind that stirred the willows made soft accompaniment of the splash of\"\n",
    "reference = \"The whisper of the wind that stirred the willows made soft accompaniment of the splash of paddle in the stream the birds sang lustily amid the gentle rustle of the garden trees, and when the thrush retired to roost the nightingale took up the tale.\"\n",
    "generated_sentence = generate_text(model, tokenizer, prompt)\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Generated Text: {generated_sentence}\")\n",
    "print(f\"Reference Text: {reference}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515d73ce",
   "metadata": {},
   "source": [
    "## 2.6 Training DistilGPT-2 with LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c7d17b",
   "metadata": {},
   "source": [
    "Loading DistilGPT-2 model and its tokenizer as for other models before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b635b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained distilgpt2 model and tokenizer\n",
    "model_name = \"distilgpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8caacb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"distilgpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd85ac82",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd32089b",
   "metadata": {},
   "source": [
    "Tokenize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be29bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply tokenization to the datasets\n",
    "tokenized_datasets = dataset_dict.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec2fe1b",
   "metadata": {},
   "source": [
    "Define LoRA config for DistilGPT-2 again as for other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50c3902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the adapter configuration (LoRA)\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"c_attn\", \"c_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded97033",
   "metadata": {},
   "source": [
    "Freezing layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f436c66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add adapters to the model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Freeze all parameters in the base model\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze only the LoRA adapter parameters\n",
    "for name, param in model.named_parameters():\n",
    "    if \"lora\" in name:\n",
    "        param.requires_grad = True  # Keep adapters trainable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337915d7",
   "metadata": {},
   "source": [
    "Training part of the model with Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde4eb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Collator\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/kaggle/working/distilgpt2-lora\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=16,\n",
    "    save_steps=5000,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=200,\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=5e-4,\n",
    "    fp16=True, \n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa25d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5487f63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6852d9",
   "metadata": {},
   "source": [
    "Save model locally and in Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0d0327",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"/kaggle/working/distilgpt2-lora\")\n",
    "tokenizer.save_pretrained(\"/kaggle/working/distilgpt2-lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468a0790",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "model_name = \"dudessa/distilgpt2-lora-finetuned\"  # Replace with your model name\n",
    "api = HfApi()\n",
    "\n",
    "# Create a new repository (if it doesn't exist already)\n",
    "api.create_repo(repo_id=model_name, exist_ok=True)\n",
    "\n",
    "# Push the model to the hub\n",
    "model.push_to_hub(model_name)\n",
    "tokenizer.push_to_hub(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ece2f26",
   "metadata": {},
   "source": [
    "Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f705d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppl_score = calculate_perplexity(model, tokenizer, test_sentences)\n",
    "print(f\"Perplexity: {ppl_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c913247",
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_score = calculate_bleu_score(model, tokenizer, test_sentences[:350]) \n",
    "print(f\"BLEU Score: {bleu_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e1940d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_score = calculate_bertscore(model, tokenizer, test_sentences[:350], device=\"cuda\", prompt_length=8)\n",
    "print(f\"Model BERT: {bert_score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed35582",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"When inquisitors punish heretics it is not with the desire to\"\n",
    "reference = \"When inquisitors punish heretics it is not with the desire to destroy them, but that they shall be converted and live.\"\n",
    "generated_sentence = generate_text(model, tokenizer, prompt)\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Generated Text: {generated_sentence}\")\n",
    "print(f\"Reference Text: {reference}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b596acf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"The most personally courageous become bullies and\"\n",
    "reference = \"The most personally courageous become bullies and the terror of the community.\"\n",
    "generated_sentence = generate_text(model, tokenizer, prompt)\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Generated Text: {generated_sentence}\")\n",
    "print(f\"Reference Text: {reference}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0532b0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"The whisper of the wind that stirred the willows made soft accompaniment of the splash of\"\n",
    "reference = \"The whisper of the wind that stirred the willows made soft accompaniment of the splash of paddle in the stream the birds sang lustily amid the gentle rustle of the garden trees, and when the thrush retired to roost the nightingale took up the tale.\"\n",
    "generated_sentence = generate_text(model, tokenizer, prompt)\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Generated Text: {generated_sentence}\")\n",
    "print(f\"Reference Text: {reference}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
